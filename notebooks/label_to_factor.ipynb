{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nutils\n",
    "from models.model_mlp_v2 import MtNet\n",
    "import copy\n",
    "import sutils\n",
    "import common as cm\n",
    "from models.model_mlp_v2 import MtNet\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import SharedArray as sa\n",
    "import torch as th\n",
    "from pytorch_lightning.callbacks import (\n",
    "    ModelCheckpoint,\n",
    "    EarlyStopping,\n",
    "    GradientAccumulationScheduler,\n",
    "    RichProgressBar,\n",
    ")\n",
    "from pytorch_lightning.loggers import wandb as wandb_logger\n",
    "import pytorch_lightning as pl\n",
    "from functools import lru_cache\n",
    "from rich.progress import track as tqdm\n",
    "from torch.utils import data\n",
    "import os\n",
    "from argparse import ArgumentParser\n",
    "import wandb\n",
    "from datetime import datetime\n",
    "from mlutils.data import DataLoaderY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = ArgumentParser()\n",
    "args.add_argument(\"--fold\", \"-f\", type=int, default=3)\n",
    "args.add_argument(\"--gpu\", \"-g\", type=int, default=0)\n",
    "args.add_argument(\n",
    "    \"--batch_size\", \"-b\", type=int, default=100, help=\"batch size multiplier\"\n",
    ")\n",
    "args.add_argument(\"--num_stocks\", \"-n\", type=int, default=100)\n",
    "args.add_argument(\"--save\", \"-s\", type=int, default=0)\n",
    "args.add_argument(\"--split\", \"-sp\", type=int, default=0)\n",
    "args.add_argument(\"--cur\", type=int, default=0)\n",
    "args.add_argument(\"--fut\", type=int, default=60)\n",
    "args = args.parse_args([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(code):\n",
    "    x = sa.attach(f\"factor_{code}\")\n",
    "    y = sa.attach(f\"label_{code}\")\n",
    "    z = sa.attach(f\"timestamp_{code}\")\n",
    "    return x, y, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = datetime.now().strftime(\"%y%m%d%H%M\")\n",
    "stk_list = cm.SELECTED_CODES\n",
    "seed = 2022\n",
    "if args.num_stocks < 100:\n",
    "    pl.seed_everything(seed)\n",
    "    idx = np.random.choice(len(stk_list), size=args.num_stocks, replace=False)\n",
    "    stk_list = [stk_list[i] for i in idx]\n",
    "print(f\"selected stk_list:\\n{stk_list}\")\n",
    "\n",
    "## Model\n",
    "labels = [0, 1, 3]\n",
    "tower_sizes = [[128, 128, 1], [32, 1], [32, 1]]\n",
    "loss_weights = [1, 0.4, 0.4]\n",
    "\n",
    "assert len(loss_weights) == len(tower_sizes)\n",
    "model_param = {\n",
    "    \"input_size\": 101,\n",
    "    \"hidden_sizes\": [64, 128],\n",
    "    \"tower_sizes\": tower_sizes,\n",
    "    \"act\": \"leakyrelu\",\n",
    "    \"dropout\": 0.4,\n",
    "    \"lr\": 1e-4,\n",
    "    \"loss_fn\": \"mse\",\n",
    "    \"weight_decay\": 1e-3, \n",
    "    \"loss_weights\": loss_weights,\n",
    "}\n",
    "\n",
    "model = MtNet(**model_param)\n",
    "\n",
    "## Logger details\n",
    "\n",
    "experiment_name = (\n",
    "    f\"f{args.fold}_n{len(stk_list)}_b{args.batch_size}_{args.cur}-{args.fut}_{[[\"ret\",\"mean\",\"var\",\"rv\"][i] for i in labels]}\"\n",
    ")\n",
    "datamodule = sutils.MTDataModule(\n",
    "    codes=stk_list,\n",
    "    labels_idx=labels,\n",
    "    fold=args.fold,\n",
    "    split=args.split,\n",
    "    batch_size=args.batch_size * 4000,\n",
    "    cur=args.cur,\n",
    "    fut=args.fut,\n",
    ")\n",
    "logger = wandb_logger.WandbLogger(\n",
    "    project=\"MultiTask\" if args.num_stocks == 100 else \"MultiTask_small\",\n",
    "    name=experiment_name,\n",
    ")\n",
    "logger.experiment.config.update(\n",
    "    {\n",
    "        \"num_stocks\": len(stk_list),\n",
    "        \"tmstamp\": timestamp,\n",
    "        \"fold\": args.fold,\n",
    "        \"release\": False,\n",
    "        \"batch_size\": args.batch_size,\n",
    "        \"lables\": labels,\n",
    "    }\n",
    ")\n",
    "logger.experiment.config.update(model_param)\n",
    "## Training details\n",
    "earlystop_callback = EarlyStopping(\n",
    "    monitor=\"valid/ic_0\",\n",
    "    patience=15,\n",
    "    mode=\"max\",\n",
    ")\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"valid/ic_0\",\n",
    "    mode=\"max\",\n",
    "    dirpath=f\"./checkpoints/{timestamp}/{args.fold}/\",\n",
    "    filename=\"model_{valid/ic_0:.3f}\",\n",
    "    save_top_k=1,\n",
    ")\n",
    "gas_callback = GradientAccumulationScheduler(scheduling={0: 10, 5: 5, 10: 1})\n",
    "\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    devices=[args.gpu],\n",
    "    callbacks=[\n",
    "        earlystop_callback,\n",
    "        checkpoint_callback,\n",
    "        RichProgressBar(),\n",
    "        gas_callback,\n",
    "        test_callback,\n",
    "    ],\n",
    "    logger=logger,\n",
    "    max_epochs=200,\n",
    "    precision=\"16-mixed\",\n",
    "    # detect_anomaly=True,\n",
    "    \n",
    ")\n",
    "\n",
    "trainer.fit(model, datamodule)\n",
    "trainer.test(model, datamodule, ckpt_path=\"best\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
